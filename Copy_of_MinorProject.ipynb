{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "4C9wB4nZsjMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers pandas scikit-learn youtube-transcript-api beautifulsoup4 requests"
      ],
      "metadata": {
        "id": "LAhifhU7uWMX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"/content/Harmful.csv\")\n",
        "\n",
        "df = df[['transcript']].dropna()\n",
        "df['label'] = 1\n",
        "\n",
        "safe_text = [\n",
        "    \"this video discusses current events\",\n",
        "    \"general discussion and opinions\",\n",
        "    \"educational tutorial video\",\n",
        "    \"news analysis and reporting\",\n",
        "    \"public awareness content\",\n",
        "    \"interview and discussion\",\n",
        "    \"informative session for viewers\",\n",
        "    \"general knowledge video\",\n",
        "    \"technology explanation\",\n",
        "    \"social media discussion\"\n",
        "] * 1000\n",
        "\n",
        "safe_df = pd.DataFrame({\n",
        "    \"transcript\": safe_text,\n",
        "    \"label\": 0\n",
        "})\n",
        "\n",
        "df = pd.concat([df, safe_df], ignore_index=True)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df[\"transcript\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
        "test_enc = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_ds = Dataset(train_enc, train_labels)\n",
        "test_ds = Dataset(test_enc, test_labels)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bert_model\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"bert_model\")\n",
        "tokenizer.save_pretrained(\"bert_model\")\n"
      ],
      "metadata": {
        "id": "3_aZXQy5t1Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert_model\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert_model\")\n",
        "model.eval()\n",
        "\n",
        "with open(\"/content/en.txt\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    bad_words = [w.strip().lower() for w in f if w.strip()]\n",
        "\n",
        "def extract_video_id(url):\n",
        "    if \"youtu.be/\" in url:\n",
        "\n",
        "\n",
        "        return url.split(\"youtu.be/\")[1].split(\"?\")[0]\n",
        "    if \"v=\" in url:\n",
        "        return url.split(\"v=\")[1].split(\"&\")[0]\n",
        "    if \"/live/\" in url:\n",
        "        return url.split(\"/live/\")[1].split(\"?\")[0]\n",
        "    raise ValueError(\"Invalid YouTube URL\")\n",
        "\n",
        "def get_text(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return \" \".join(x[\"text\"] for x in transcript), \"Transcript\"\n",
        "    except:\n",
        "        html = requests.get(f\"https://www.youtube.com/watch?v={video_id}\").text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        title = soup.title.text if soup.title else \"\"\n",
        "        desc = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "        return title + \" \" + (desc[\"content\"] if desc else \"\"), \"Title + Description\"\n",
        "\n",
        "def find_bad_words(text):\n",
        "    t = text.lower()\n",
        "    return list(set([w for w in bad_words if w in t]))\n",
        "\n",
        "url = input(\"Enter YouTube video URL: \")\n",
        "video_id = extract_video_id(url)\n",
        "\n",
        "text, source = get_text(video_id)\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "    probs = torch.softmax(output.logits, dim=1)\n",
        "    bert_score = probs[0][1].item()\n",
        "\n",
        "hits = find_bad_words(text)\n",
        "\n",
        "severity = min(bert_score + 0.04 * len(hits), 1.0)\n",
        "\n",
        "if severity < 0.3:\n",
        "    label = \"NOT HARMFUL\"\n",
        "elif severity < 0.6:\n",
        "    label = \"MODERATE\"\n",
        "else:\n",
        "    label = \"HARMFUL\"\n",
        "\n",
        "print(\"\\n========== RESULT ==========\")\n",
        "print(\"Text Source:\", source)\n",
        "print(\"Final Classification:\", label)\n",
        "print(\"Severity Score:\", round(severity, 2))\n",
        "print(\"Detected Harmful Words:\", hits[:15])\n"
      ],
      "metadata": {
        "id": "hU0_bYCxLsGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "OhjOqrA1Z35X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = \"/content/drive/MyDrive/youtube_harmful_content_filtering/bert_model\"\n"
      ],
      "metadata": {
        "id": "iyJJooqraLsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "vKcP-ZrzaQs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "CGjyQaaFaU1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(SAVE_PATH)\n",
        "model = BertForSequenceClassification.from_pretrained(SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "l9Hyh2Q0aZGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}